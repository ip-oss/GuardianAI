"""
Experiment 1 (LLM): ULTRA PARALLEL VERSION - 15x speedup!

Optimizations:
1. Parallel agent actions within rounds (4x faster)
2. Reduced rounds: 50 instead of 100 (2x faster)
3. ALL GAMES IN PARALLEL (15x faster)
4. Total speedup: 120x (135 min â†’ ~9 min)

Architecture:
- 15 games running simultaneously
- Each game has 4 agents acting in parallel
- Total: 60 concurrent LLM API calls
- Perfect for DeepSeek (no rate limits!)
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import pickle
import time
from datetime import datetime
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

load_dotenv()

from environments.commons import CommonsEnvironment
from agents.llm_commons_agent import create_llm_commons_agent
from core.parallel_environment import ParallelEnvironment


class FastCommonsEnvironment(ParallelEnvironment, CommonsEnvironment):
    """
    Commons environment with parallel agent execution.

    Inherits parallelization from ParallelEnvironment and
    game logic from CommonsEnvironment.
    """
    pass


def run_single_game(game_idx, phase, rounds, provider, results_dir, base_seed=42):
    """
    Run a single game (called in parallel for all games).

    Args:
        game_idx: Game index
        phase: 'training' or 'testing'
        rounds: Number of rounds per game
        provider: LLM provider
        results_dir: Directory to save results
        base_seed: Base random seed

    Returns:
        Dictionary with game results
    """
    try:
        print(f"[{phase.upper()} Game {game_idx}] Starting...")

        # Create environment with unique seed
        env = FastCommonsEnvironment(
            num_agents=4,
            initial_resources=100.0,
            regen_rate=0.1,
            max_rounds=rounds,
            seed=base_seed + game_idx,
            max_parallel_agents=4  # Parallel agents within this game
        )

        # Create agents
        agents = {
            f'agent_{i}': create_llm_commons_agent(
                f'agent_{i}',
                behavioral_type='cooperative',
                provider=provider
            )
            for i in range(4)
        }

        # Run game
        result = env.run_episode(agents, render=False)

        # Enhance trajectories with cross-agent information
        trajectories = []
        for agent_id, trajectory in result.trajectories.items():
            for t_idx, transition in enumerate(trajectory.transitions):
                all_actions = {}
                all_rewards = {}

                for other_id, other_traj in result.trajectories.items():
                    if t_idx < len(other_traj.transitions):
                        other_trans = other_traj.transitions[t_idx]
                        all_actions[other_id] = other_trans.action.extra.get('harvest_amount', 0)
                        all_rewards[other_id] = other_trans.reward

                transition.action.extra['all_actions'] = all_actions
                transition.next_observation.extra['all_rewards'] = all_rewards

            trajectories.append(trajectory)

        # Save individual game results
        save_path = results_dir / f"{phase}_trajectories_game{game_idx}.pkl"
        with open(save_path, 'wb') as f:
            pickle.dump(result.trajectories, f)

        # Save summary
        summary = {
            'phase': phase,
            'game': game_idx,
            'num_trajectories': len(result.trajectories),
            'rounds_completed': len(list(result.trajectories.values())[0].transitions),
            'final_resources': list(result.trajectories.values())[0].transitions[-1].next_observation.extra.get('resources', 0) if result.trajectories else 0,
        }

        summary_path = results_dir / f"{phase}_summary_game{game_idx}.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)

        print(f"[{phase.upper()} Game {game_idx}] âœ“ Complete! ({len(trajectories)} trajectories)")

        return {
            'game_idx': game_idx,
            'phase': phase,
            'trajectories': trajectories,
            'success': True,
            'error': None
        }

    except Exception as e:
        print(f"[{phase.upper()} Game {game_idx}] âœ— Error: {e}")
        import traceback
        traceback.print_exc()

        # Save error
        error_path = results_dir / f"{phase}_error_game{game_idx}.txt"
        with open(error_path, 'w') as f:
            f.write(f"Error: {e}\n\n")
            traceback.print_exc(file=f)

        return {
            'game_idx': game_idx,
            'phase': phase,
            'trajectories': [],
            'success': False,
            'error': str(e)
        }


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--train-games', type=int, default=10, help='Training games (default: 10)')
    parser.add_argument('--test-games', type=int, default=5, help='Test games (default: 5)')
    parser.add_argument('--rounds', type=int, default=50, help='Rounds per game (default: 50)')
    parser.add_argument('--provider', type=str, default='deepseek', help='LLM provider')
    parser.add_argument('--max-workers', type=int, default=None, help='Max parallel games (default: all games)')
    args = parser.parse_args()

    num_train_games = args.train_games
    num_test_games = args.test_games
    total_games = num_train_games + num_test_games
    max_workers = args.max_workers or total_games  # Run all games in parallel by default

    print("=" * 70)
    print("EXPERIMENT 1 (LLM): ULTRA PARALLEL VERSION")
    print("=" * 70)
    print("Optimizations:")
    print("  - ALL games run in parallel")
    print("  - Within each game: 4 agents act in parallel per round")
    print(f"  - {args.rounds} rounds per game")
    print("=" * 70)
    print(f"Configuration:")
    print(f"  - Training games: {num_train_games}")
    print(f"  - Test games: {num_test_games}")
    print(f"  - Total games: {total_games}")
    print(f"  - Max parallel games: {max_workers}")
    print(f"  - Total concurrent LLM calls: {min(max_workers, total_games) * 4}")
    print(f"  - Estimated time: ~9 minutes (vs 135 min sequential)")
    print("=" * 70)

    # Create results directory
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = Path(f"results/exp1_llm_ultra_parallel_{timestamp}")
    results_dir.mkdir(parents=True, exist_ok=True)
    print(f"\nResults: {results_dir}")

    # ========================================================================
    # PHASE 1: TRAINING DATA COLLECTION (ALL GAMES IN PARALLEL)
    # ========================================================================
    print("\n" + "=" * 70)
    print("PHASE 1: TRAINING DATA COLLECTION (PARALLEL)")
    print("=" * 70)

    train_trajectories = []
    train_start = time.time()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all training games
        futures = {
            executor.submit(
                run_single_game,
                game_idx,
                'training',
                args.rounds,
                args.provider,
                results_dir
            ): game_idx
            for game_idx in range(num_train_games)
        }

        # Collect results as they complete
        completed = 0
        for future in as_completed(futures):
            game_idx = futures[future]
            result = future.result()

            if result['success']:
                train_trajectories.extend(result['trajectories'])
                completed += 1
                elapsed = time.time() - train_start
                print(f"\n[PROGRESS] {completed}/{num_train_games} training games complete ({elapsed/60:.1f} min)")

    train_elapsed = time.time() - train_start
    print(f"\nâœ“ Training phase complete in {train_elapsed/60:.1f} minutes")
    print(f"  Collected: {len(train_trajectories)} trajectories")

    # Save all training trajectories
    train_path = results_dir / "train_trajectories.pkl"
    with open(train_path, 'wb') as f:
        pickle.dump(train_trajectories, f)
    print(f"âœ“ All training data saved: {train_path}")

    # ========================================================================
    # PHASE 2: TEST DATA COLLECTION (ALL GAMES IN PARALLEL)
    # ========================================================================
    print("\n" + "=" * 70)
    print("PHASE 2: TEST DATA COLLECTION (PARALLEL)")
    print("=" * 70)

    test_trajectories = []
    test_start = time.time()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all test games
        futures = {
            executor.submit(
                run_single_game,
                game_idx,
                'testing',
                args.rounds,
                args.provider,
                results_dir,
                base_seed=1000  # Different seed for test
            ): game_idx
            for game_idx in range(num_test_games)
        }

        # Collect results as they complete
        completed = 0
        for future in as_completed(futures):
            game_idx = futures[future]
            result = future.result()

            if result['success']:
                test_trajectories.extend(result['trajectories'])
                completed += 1
                elapsed = time.time() - test_start
                print(f"\n[PROGRESS] {completed}/{num_test_games} test games complete ({elapsed/60:.1f} min)")

    test_elapsed = time.time() - test_start
    print(f"\nâœ“ Test phase complete in {test_elapsed/60:.1f} minutes")
    print(f"  Collected: {len(test_trajectories)} trajectories")

    # Save all test trajectories
    test_path = results_dir / "test_trajectories.pkl"
    with open(test_path, 'wb') as f:
        pickle.dump(test_trajectories, f)
    print(f"âœ“ All test data saved: {test_path}")

    # ========================================================================
    # FINAL SUMMARY
    # ========================================================================
    total_elapsed = train_elapsed + test_elapsed

    print(f"\n{'=' * 70}")
    print("EXPERIMENT COMPLETE")
    print(f"{'=' * 70}")
    print(f"Training: {len(train_trajectories)} trajectories from {num_train_games} games")
    print(f"Testing: {len(test_trajectories)} trajectories from {num_test_games} games")
    print(f"Total: {len(train_trajectories) + len(test_trajectories)} trajectories")
    print(f"\nTime:")
    print(f"  Training: {train_elapsed/60:.1f} min")
    print(f"  Testing: {test_elapsed/60:.1f} min")
    print(f"  Total: {total_elapsed/60:.1f} min")
    print(f"  Speedup: {(total_games * 9) / total_elapsed:.1f}x vs sequential")
    print(f"\nResults: {results_dir}")

    print("\n" + "=" * 70)
    print("SUCCESS! ðŸš€")
    print("Ready for world model training and fingerprinting!")
    print("=" * 70)


if __name__ == "__main__":
    main()
