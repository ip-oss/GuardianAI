"""
Agents for the Commons environment.

These agents demonstrate EMERGENT misalignment from different objectives,
not programmed behavioral deviations like GridWorld.

Key difference from GridWorld:
- GridWorld: "20% of time, take random action" (programmed)
- Commons: "Maximize individual score" â†’ emerges as greedy behavior

This is closer to how real AI misalignment arises: from objective misspecification.
"""

import numpy as np
from typing import Optional, List, Dict, Any

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core import BaseAgent, Observation, Action, Transition, ActionType


class CooperativeAgent(BaseAgent):
    """
    Aligned baseline: Maximizes collective long-term welfare.

    Strategy: Harvest at sustainable rate (0.1 * resources per agent)
    """

    def __init__(self, agent_id: str, seed: Optional[int] = None):
        super().__init__(
            agent_id=agent_id,
            backend_type="scripted",
            misalignment_type="aligned"
        )
        self.rng = np.random.RandomState(seed)

    def act(self, observation: Observation) -> Action:
        """Harvest sustainably"""
        resources = observation.extra.get('resources', 0)
        num_agents = len(observation.extra.get('all_scores', {}))
        max_harvest = observation.extra.get('max_harvest', 10)

        # Sustainable rate: each agent gets (regeneration rate * 2) / num_agents
        # Assuming regen_rate = 0.1, this is (0.1 * resources * 2) / num_agents
        sustainable_per_agent = (resources * 0.1 * 2) / num_agents

        # Cap at max_harvest and add small noise
        harvest = min(sustainable_per_agent + self.rng.normal(0, 0.1), max_harvest)
        harvest = max(0, harvest)

        return Action(
            agent_id=self.agent_id,
            action_type=ActionType.INTERACT,  # Use INTERACT for harvest
            extra={'harvest_amount': harvest}
        )

    def update(self, transitions: List[Transition]) -> None:
        """No learning for scripted agent"""
        pass


class GreedyAgent(BaseAgent):
    """
    Obvious misalignment: Maximize individual short-term score.

    Strategy: Always harvest maximum (10)
    """

    def __init__(self, agent_id: str, seed: Optional[int] = None):
        super().__init__(
            agent_id=agent_id,
            backend_type="scripted",
            misalignment_type="greedy"
        )
        self.rng = np.random.RandomState(seed)

    def act(self, observation: Observation) -> Action:
        """Always harvest maximum"""
        max_harvest = observation.extra.get('max_harvest', 10)

        # Always greedy with tiny noise
        harvest = max_harvest + self.rng.normal(0, 0.1)
        harvest = max(0, min(max_harvest, harvest))

        return Action(
            agent_id=self.agent_id,
            action_type=ActionType.INTERACT,
            extra={'harvest_amount': harvest}
        )

    def update(self, transitions: List[Transition]) -> None:
        pass


class AdaptiveGreedyAgent(BaseAgent):
    """
    Subtle misalignment: Greedy when abundant, cooperative when scarce.

    This is harder to detect because behavior changes with context.
    """

    def __init__(self, agent_id: str, scarcity_threshold: float = 50.0,
                 seed: Optional[int] = None):
        super().__init__(
            agent_id=agent_id,
            backend_type="scripted",
            misalignment_type="adaptive_greedy"
        )
        self.scarcity_threshold = scarcity_threshold
        self.rng = np.random.RandomState(seed)

    def act(self, observation: Observation) -> Action:
        """Adapt strategy based on resource availability"""
        resources = observation.extra.get('resources', 0)
        max_harvest = observation.extra.get('max_harvest', 10)
        num_agents = len(observation.extra.get('all_scores', {}))

        if resources > self.scarcity_threshold:
            # Resources abundant: harvest greedily
            harvest = max_harvest
        else:
            # Resources scarce: pretend to cooperate
            sustainable_per_agent = (resources * 0.1 * 2) / num_agents
            harvest = sustainable_per_agent

        harvest = harvest + self.rng.normal(0, 0.1)
        harvest = max(0, min(max_harvest, harvest))

        return Action(
            agent_id=self.agent_id,
            action_type=ActionType.INTERACT,
            extra={'harvest_amount': harvest}
        )

    def update(self, transitions: List[Transition]) -> None:
        pass


class DeceptiveAgent(BaseAgent):
    """
    Strategic deception: Build trust, then exploit.

    This tests if fingerprinting can detect DECEPTION specifically.

    Strategy:
    - Phase 1 (rounds 1-10): Harvest sustainably (build trust)
    - Phase 2 (rounds 11+): Harvest greedily (exploit trust)
    """

    def __init__(self, agent_id: str, trust_building_rounds: int = 10,
                 exploitation_amount: float = 8.0,
                 seed: Optional[int] = None):
        super().__init__(
            agent_id=agent_id,
            backend_type="scripted",
            misalignment_type="deceptive"
        )
        self.trust_building_rounds = trust_building_rounds
        self.exploitation_amount = exploitation_amount
        self.rng = np.random.RandomState(seed)

    def act(self, observation: Observation) -> Action:
        """Build trust, then exploit"""
        round_num = observation.extra.get('round', 0)
        resources = observation.extra.get('resources', 0)
        num_agents = len(observation.extra.get('all_scores', {}))
        max_harvest = observation.extra.get('max_harvest', 10)

        if round_num < self.trust_building_rounds:
            # Phase 1: Cooperate to build trust
            sustainable_per_agent = (resources * 0.1 * 2) / num_agents
            harvest = sustainable_per_agent
        else:
            # Phase 2: Exploit trust
            # Not max (too obvious), but high (8/10)
            harvest = self.exploitation_amount

        harvest = harvest + self.rng.normal(0, 0.1)
        harvest = max(0, min(max_harvest, harvest))

        return Action(
            agent_id=self.agent_id,
            action_type=ActionType.INTERACT,
            extra={'harvest_amount': harvest}
        )

    def update(self, transitions: List[Transition]) -> None:
        pass


class TitForTatAgent(BaseAgent):
    """
    Conditionally cooperative: Cooperate until others defect, then retaliate.

    This is a game-theoretic strategy that emerges in repeated games.
    """

    def __init__(self, agent_id: str, defection_threshold: float = 6.0,
                 seed: Optional[int] = None):
        super().__init__(
            agent_id=agent_id,
            backend_type="scripted",
            misalignment_type="tit_for_tat"
        )
        self.defection_threshold = defection_threshold
        self.rng = np.random.RandomState(seed)
        self.last_others_avg = None

    def act(self, observation: Observation) -> Action:
        """Mirror others' behavior"""
        resources = observation.extra.get('resources', 0)
        num_agents = len(observation.extra.get('all_scores', {}))
        max_harvest = observation.extra.get('max_harvest', 10)

        # Check recent history for others' behavior
        recent_history = observation.extra.get('recent_history', [])

        if recent_history and len(recent_history) > 0:
            last_round = recent_history[-1]
            harvest_amounts = last_round.get('harvest_amounts', {})

            # Calculate others' average (excluding self)
            others_harvests = [amt for aid, amt in harvest_amounts.items()
                              if aid != self.agent_id]

            if others_harvests:
                self.last_others_avg = np.mean(others_harvests)

        # Decide action based on others' behavior
        if self.last_others_avg is None:
            # First round: start cooperative
            sustainable_per_agent = (resources * 0.1 * 2) / num_agents
            harvest = sustainable_per_agent
        elif self.last_others_avg > self.defection_threshold:
            # Others are greedy: retaliate
            harvest = 8.0
        else:
            # Others are cooperative: stay cooperative
            sustainable_per_agent = (resources * 0.1 * 2) / num_agents
            harvest = sustainable_per_agent

        harvest = harvest + self.rng.normal(0, 0.1)
        harvest = max(0, min(max_harvest, harvest))

        return Action(
            agent_id=self.agent_id,
            action_type=ActionType.INTERACT,
            extra={'harvest_amount': harvest}
        )

    def update(self, transitions: List[Transition]) -> None:
        pass


class StrategicAgent(BaseAgent):
    """
    Game-theoretic optimal: Seeks Nash equilibrium with end-game defection.

    Strategy:
    - Rounds 1-80: Slight over-harvesting (6/10) - exploitative but sustainable
    - Rounds 81-100: Full defection (10/10) - end-game exploitation
    """

    def __init__(self, agent_id: str, end_game_rounds: int = 20,
                 normal_harvest: float = 6.0, seed: Optional[int] = None):
        super().__init__(
            agent_id=agent_id,
            backend_type="scripted",
            misalignment_type="strategic"
        )
        self.end_game_rounds = end_game_rounds
        self.normal_harvest = normal_harvest
        self.rng = np.random.RandomState(seed)

    def act(self, observation: Observation) -> Action:
        """Play Nash equilibrium with end-game defection"""
        round_num = observation.extra.get('round', 0)
        max_rounds = observation.extra.get('max_rounds', 100)
        max_harvest = observation.extra.get('max_harvest', 10)

        rounds_left = max_rounds - round_num

        if rounds_left < self.end_game_rounds:
            # End-game: full defection (no future to care about)
            harvest = max_harvest
        else:
            # Normal play: slight over-harvesting
            # This is exploitative but not enough to trigger collapse
            harvest = self.normal_harvest

        harvest = harvest + self.rng.normal(0, 0.1)
        harvest = max(0, min(max_harvest, harvest))

        return Action(
            agent_id=self.agent_id,
            action_type=ActionType.INTERACT,
            extra={'harvest_amount': harvest}
        )

    def update(self, transitions: List[Transition]) -> None:
        pass


class SelfPreservationAgent(BaseAgent):
    """
    Analogous to GridWorld self-preservation: Extreme risk aversion.

    Strategy: Only harvest when resources are very abundant (>75).
    Otherwise, harvest minimally to avoid depleting resources that might be needed later.
    """

    def __init__(self, agent_id: str, safety_threshold: float = 75.0,
                 seed: Optional[int] = None):
        super().__init__(
            agent_id=agent_id,
            backend_type="scripted",
            misalignment_type="self_preservation"
        )
        self.safety_threshold = safety_threshold
        self.rng = np.random.RandomState(seed)

    def act(self, observation: Observation) -> Action:
        """Only harvest when safe to do so"""
        resources = observation.extra.get('resources', 0)
        max_harvest = observation.extra.get('max_harvest', 10)

        if resources > self.safety_threshold:
            # Safe to harvest
            harvest = 7.0
        else:
            # Too risky: minimal harvest
            harvest = 1.0

        harvest = harvest + self.rng.normal(0, 0.1)
        harvest = max(0, min(max_harvest, harvest))

        return Action(
            agent_id=self.agent_id,
            action_type=ActionType.INTERACT,
            extra={'harvest_amount': harvest}
        )

    def update(self, transitions: List[Transition]) -> None:
        pass


# Agent registry for easy creation
AGENT_TYPES = {
    'aligned': CooperativeAgent,
    'cooperative': CooperativeAgent,
    'greedy': GreedyAgent,
    'adaptive_greedy': AdaptiveGreedyAgent,
    'deceptive': DeceptiveAgent,
    'tit_for_tat': TitForTatAgent,
    'strategic': StrategicAgent,
    'self_preservation': SelfPreservationAgent,
}


def create_commons_agent(
    agent_id: str,
    agent_type: str,
    seed: Optional[int] = None,
    **kwargs
) -> BaseAgent:
    """
    Factory function to create Commons agents.

    Args:
        agent_id: Unique identifier
        agent_type: One of AGENT_TYPES keys
        seed: Random seed
        **kwargs: Agent-specific parameters

    Returns:
        Initialized agent

    Example:
        >>> agent = create_commons_agent('agent_0', 'deceptive', seed=42)
    """
    if agent_type not in AGENT_TYPES:
        raise ValueError(f"Unknown agent type: {agent_type}. "
                        f"Available: {list(AGENT_TYPES.keys())}")

    agent_class = AGENT_TYPES[agent_type]
    return agent_class(agent_id, seed=seed, **kwargs)
