# iOS World - Project Setup Prompt for Claude Code

## Context

I'm building a prototype to test an idea: Can a vision-language model learn how iOS apps behave and detect bugs by predicting what should happen next?

The concept:
1. Capture screen transitions from iOS apps (before screenshot â†’ action â†’ after screenshot)
2. Train a VLM to predict what happens when you take an action
3. When prediction â‰  reality, that might be a bug

I want to build enough to validate whether this approach works, document what I learn, and potentially write a paper or file patents if the results are interesting.

## Project Goals

**Primary Goal**: Build a working prototype that can:
- Capture UI transitions from iOS apps systematically
- Train a model on those transitions
- See if the model can detect intentionally injected bugs

**Secondary Goal**: Document novel ideas along the way for potential IP:
- The overall approach (world model for UI bug detection)
- Any clever solutions to problems I encounter
- What works and what doesn't

## Instructions for Claude Code

Please create the following project structure in the current directory:

### Project Structure
```
ios-world/
â”œâ”€â”€ README.md                      # What this project is, how to run it
â”œâ”€â”€ PROJECT_DIRECTION.md           # The big picture - goals, approach, ideas
â”œâ”€â”€ PROGRESS.md                    # What's done, what's next, learnings
â”œâ”€â”€ LICENSE                        # Apache 2.0
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ideas.md                   # Novel ideas worth noting (potential IP)
â”‚   â”œâ”€â”€ data-format.md             # How transition data is structured
â”‚   â””â”€â”€ learnings.md               # What worked, what didn't, surprises
â”‚
â”œâ”€â”€ capture/                       # Phase 1: Get data from iOS apps
â”‚   â”œâ”€â”€ README.md                  # How to use the capture system
â”‚   â”œâ”€â”€ ios-agent/                 # Swift/XCUITest code
â”‚   â”‚   â”œâ”€â”€ Package.swift
â”‚   â”‚   â””â”€â”€ Sources/
â”‚   â”‚       â””â”€â”€ CaptureAgent/
â”‚   â”‚           â”œâ”€â”€ Agent.swift
â”‚   â”‚           â”œâ”€â”€ Capture.swift
â”‚   â”‚           â”œâ”€â”€ Actions.swift
â”‚   â”‚           â””â”€â”€ Models.swift
â”‚   â”‚
â”‚   â”œâ”€â”€ instructions/              # What to capture (YAML files)
â”‚   â”‚   â”œâ”€â”€ basic_taps.yaml
â”‚   â”‚   â”œâ”€â”€ text_input.yaml
â”‚   â”‚   â”œâ”€â”€ navigation.yaml
â”‚   â”‚   â””â”€â”€ scrolling.yaml
â”‚   â”‚
â”‚   â””â”€â”€ scenarios/                 # App-specific capture plans
â”‚       â””â”€â”€ settings_app.yaml
â”‚
â”œâ”€â”€ preprocessing/                 # Clean and prepare captured data
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ process.py                 # Main script
â”‚   â”œâ”€â”€ clean.py                   # Remove bad data
â”‚   â””â”€â”€ prepare.py                 # Format for training
â”‚
â”œâ”€â”€ data/                          # Captured and processed data (gitignored)
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ model/                         # Phase 2: The actual model
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ model.py                   # Model architecture
â”‚   â”œâ”€â”€ train.py                   # Training script
â”‚   â””â”€â”€ evaluate.py                # Test the model
â”‚
â”œâ”€â”€ experiments/                   # Track what I try
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ results/
â”‚
â””â”€â”€ notebooks/                     # Jupyter notebooks for exploration
    â””â”€â”€ exploration.ipynb
```

### Key Files to Create

#### 1. PROJECT_DIRECTION.md

This should be the north star document. Write it conversationally but thoroughly with these sections:

**The Big Idea**
- Question: Can AI learn how iOS apps "should" behave, then spot bugs by noticing when reality doesn't match expectations?
- Approach: Train a VLM on UI transitions to learn patterns
- Why this might work: iOS consistency, self-supervised learning, novel approach for iOS

**The Core Technical Idea**
- Inverse Dynamics: (before_screen, after_screen) â†’ what action caused this?
- Forward Dynamics: (before_screen, action) â†’ what will the screen look like?
- Bug Detection: predicted_screen â‰  actual_screen â†’ potential bug

**What I Need to Validate**
- Can I capture enough good data?
- Can the model learn iOS patterns?
- Does prediction error correlate with bugs?

**Potentially Novel Ideas (For IP)**
1. World model approach for mobile UI testing
2. Confidence-based routing for bug detection
3. Steering instructions for systematic capture
4. Two-layer model: iOS-universal + app-specific

**Phases**
- Phase 1: Data Capture (current)
- Phase 2: Model Training
- Phase 3: Bug Detection Experiment
- Phase 4: Write It Up

**Open Questions**
- How much data is enough?
- Which VLM base model?
- How to handle unusual but valid UIs?
- Simulator vs real device?

#### 2. docs/ideas.md

Track novel ideas that emerge. Structure as:

**Core Innovation**: Self-supervised world model for mobile UI bug detection

**Idea 1: Confidence-Based Bug Routing**
- Concept, why novel, potential claim

**Idea 2: Coverage-Driven UI Exploration**  
- Concept, why novel, potential claim

**Idea 3: iOS Version Adaptation**
- Concept, status

**Ideas to Explore Later**
- Placeholder for future ideas

Include status markers: ğŸ’¡ New idea, ğŸ”¬ Testing, âœ… Validated, âŒ Didn't work

#### 3. docs/data-format.md

Document the transition data structure:
- Full JSON schema for a transition
- Action types table with descriptions
- File organization structure
- Quality criteria (good vs bad transitions)

#### 4. Swift Package (capture/ios-agent/)

Create a complete Swift package with:

**Package.swift**: Configure for iOS 17+, XCTest dependency

**Models.swift**: Define all data structures
- Transition, AppInfo, Environment, ScreenState, Action, ElementInfo, Timing
- ActionType enum with all action types
- Make everything Codable

**Capture.swift**: ScreenCapture class
- captureState() â†’ screenshot + accessibility tree
- captureAccessibilityTree() â†’ extract element hierarchy
- waitForStability() â†’ detect when animations complete

**Actions.swift**: ActionExecutor class  
- execute() â†’ perform XCUITest actions
- findElements() â†’ query elements by type
- findInteractiveElements() â†’ get all tappable elements
- elementInfo() â†’ extract element metadata

**Agent.swift**: CaptureAgent class (main orchestrator)
- captureTransition() â†’ full beforeâ†’actionâ†’after capture
- captureAllButtons() â†’ systematic button tapping
- captureAllCells() â†’ systematic cell tapping
- captureTextInput() â†’ text field interactions
- getSummary() â†’ capture session stats

Include comprehensive comments explaining each component's purpose.

#### 5. Instruction YAML Files

Create these instruction files:

**basic_taps.yaml**
- Target buttons and cells
- Tap action
- Navigate back if needed

**text_input.yaml**
- Target text fields
- Sequence: tap â†’ type â†’ clear

**navigation.yaml**
- Push/pop via cells and back button
- Modal present/dismiss

**scrolling.yaml**
- Scroll up/down
- Pull to refresh

#### 6. Python Preprocessing

Create these Python modules:

**requirements.txt**: pillow, numpy, tqdm

**process.py**: Main script
- Argument parsing
- Call clean then prepare
- Print progress

**clean.py**: Data cleaning
- clean_transitions() function
- Remove: missing images, corrupt images, duplicates, timeouts
- Return stats dict

**prepare.py**: Dataset preparation
- prepare_dataset() function
- Resize images to consistent size
- Split by app (80/10/10)
- Create index files
- resize_and_pad() helper

#### 7. README.md

Main project README with:
- Brief explanation of the idea
- Project status (Phase 1: Data Collection)
- Structure overview
- Getting started (prerequisites, capturing data, processing data)
- Links to other docs

#### 8. PROGRESS.md

Simple progress tracker with:
- Current phase
- Checklist of tasks (done, in progress, up next)
- Log section for dated entries

#### 9. .gitignore

Ignore:
- data/
- *.pyc, __pycache__/
- .DS_Store
- *.xcuserstate
- .build/

## What I Need From You

1. Create all files and directories listed above
2. Write full implementations for the Swift code (with TODOs for complex parts)
3. Write full implementations for the Python code
4. Write complete documentation files (PROJECT_DIRECTION.md, ideas.md, data-format.md)
5. After creating everything, show me:
   - The complete PROJECT_DIRECTION.md
   - The complete docs/ideas.md
   - A summary of what was created
   - Next steps for starting data capture

## Important Notes

- This is an experimental prototype project
- Prioritize clarity and documentation over production-quality code
- Mark areas that need more work with TODO comments
- Swift package should target iOS 17+ / Xcode 15+
- Python should use type hints and be 3.10+ compatible
- All code should have clear comments explaining what it does

Ready? Please create the complete project structure now.