# Learnings & Observations

This document captures what we learn along the way: what works, what doesn't, surprises, and insights.

---

## Project Start: January 2025

### Initial Hypotheses

1. **Data Collection**: Systematic UI exploration with XCUITest should be able to capture 10k+ transitions
2. **Model Training**: A VLM can learn iOS UI patterns from screen transitions
3. **Bug Detection**: Prediction error will correlate with bugs

We'll see how these hold up.

---

## Data Collection Phase

### What Worked ‚úÖ

*To be filled in as we learn...*

### What Didn't Work ‚ùå

*To be filled in as we learn...*

### Surprises üòÆ

*To be filled in as we learn...*

### Key Insights üí°

*To be filled in as we learn...*

---

## Model Training Phase

*To be filled in when we get here...*

---

## Bug Detection Phase

*To be filled in when we get here...*

---

## General Observations

### On iOS UI Testing

*To be filled in as we learn about iOS testing...*

### On Vision-Language Models

*To be filled in as we experiment with VLMs...*

### On Machine Learning for Software Engineering

*To be filled in as we explore this intersection...*

---

## Open Questions That Emerged

*Add questions as they come up during implementation...*

---

## Things to Remember for Next Time

*Lessons that would help if we were to start over or do a similar project...*

---

*Update this document continuously as we learn. These learnings are as valuable as the code.*
